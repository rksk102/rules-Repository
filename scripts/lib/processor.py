#!/usr/bin/env python3
import os
import sys
import re
import ipaddress
import base64

# =========================
# é…ç½®ä¸å…¨å±€å˜é‡
# =========================
SOURCE_DIR = "rulesets"

STATS = {
    "files_processed": 0,
    "base64_decoded": 0,
    "original_lines": 0,
    "valid_lines": 0,
    "errors": []
}

# =========================
# GitHub Actions è¾…åŠ©å‡½æ•°
# =========================
def gh_group_start(title):
    print(f"::group::ğŸ› ï¸ {title}")
    sys.stdout.flush()

def gh_group_end():
    print("::endgroup::")
    sys.stdout.flush()

def print_step(msg):
    print(f"\033[1;34m[PROC]\033[0m {msg}")

def print_success(msg):
    print(f"\033[1;32m[OK]\033[0m   {msg}")

def gh_error(msg, file=None):
    msg_str = f"::error::{msg}" if not file else f"::error file={file}::{msg}"
    print(msg_str)
    STATS["errors"].append(msg)

# =========================
# æ ¸å¿ƒé€»è¾‘ (æ¥æºäºä½ ä¸Šä¼ çš„æ–‡ä»¶)
# =========================

def decode_if_base64(content):
    """å°è¯•æ¢æµ‹å¹¶è§£ç  Base64 å†…å®¹"""
    s = content.strip()
    if ' ' not in s and len(s) % 4 == 0 and len(s) > 20:
        try:
            decoded = base64.b64decode(s).decode('utf-8', errors='ignore')
            if '\n' in decoded or '\r' in decoded:
                STATS["base64_decoded"] += 1
                return decoded
        except Exception:
            pass
    return content

def parse_content_to_list(text):
    """
    æå–æ–‡æœ¬ä¸­çš„æœ‰æ•ˆè¡Œï¼Œæ”¯æŒ Yaml Payload æå–
    """
    lines = []
    text = decode_if_base64(text)
    
    # ç®€å•çš„ YAML payload æ¢æµ‹
    has_payload_keyword = re.search(r'^[\s]*payload:', text, re.MULTILINE | re.IGNORECASE)
    in_payload = False
    
    raw_lines = text.splitlines()
    STATS["original_lines"] += len(raw_lines)
    
    for line in raw_lines:
        line = line.strip()
        if not line: continue
        if line.startswith('#') or line.startswith('!') or line.startswith('//'): continue
        
        # å»é™¤è¡Œå°¾æ³¨é‡Š
        if ' #' in line: line = line.split(' #', 1)[0].strip()
        if '#' in line and not has_payload_keyword: # ç®€å•é˜²æ­¢è¯¯ä¼¤ url anchor
             line = line.split('#', 1)[0].strip()

        # å¤„ç† Clash YAML ç»“æ„ (payload:)
        if has_payload_keyword:
            if re.match(r'^[\s]*payload:', line, re.IGNORECASE):
                in_payload = True
                # æ£€æŸ¥å†…è” [a, b]
                m_inline = re.match(r'^[\s]*payload:\s*\[(.*)\]', line, re.IGNORECASE)
                if m_inline:
                    parts = m_inline.group(1).split(',')
                    for p in parts:
                        p = p.strip().strip("'").strip('"')
                        if p: lines.append(p)
                continue
            
            if in_payload:
                if line.startswith('-'):
                    val = line[1:].strip().strip("'").strip('"')
                    if val: lines.append(val)
                elif ':' in line:
                    in_payload = False # é‡åˆ°ä¸‹ä¸€ä¸ª key
                continue

        # æ™®é€šåˆ—è¡¨å¤„ç† (- domain)
        if line.startswith('- '):
            line = line[2:].strip()
        
        line = line.strip("'").strip('"')
        if line:
            lines.append(line)
            
    return lines

def process_domain_list(raw_list):
    """
    æ¸…æ´—åŸŸåï¼šè½¬å°å†™ã€å»å‰ç¼€ã€å»é‡ã€æ’åº
    """
    valid_domains = set()
    re_ip = re.compile(r'^\d{1,3}(\.\d{1,3}){3}$') # ç®€å•è¿‡æ»¤çº¯IP
    
    for item in raw_list:
        s = item.lower().strip()
        
        # Adblock è½¬æ¢ ||example.com^ -> example.com
        if s.startswith('||'): s = s[2:]
        if s.endswith('^'): s = s[:-1]
        
        # å»é™¤é€šé…ç¬¦
        s = re.sub(r'^(\*\.|\+\.|\.)', '', s)
        
        # ä¸¢å¼ƒè·¯å¾„å’Œç«¯å£
        if '/' in s: s = s.split('/')[0]
        if ':' in s: s = s.split(':')[0]
            
        if not s or '.' not in s: continue
        if re_ip.match(s): continue 
        
        # åˆæ³•æ€§æ£€æŸ¥
        if not all(c.isalnum() or c in '-._' for c in s): continue
            
        valid_domains.add(s)
        
    return sorted(list(valid_domains))

def process_ip_list(raw_list):
    """
    æ¸…æ´— IPï¼šæ ‡å‡†åŒ–ã€åˆå¹¶ç½‘æ®µ (Collapsing)
    """
    ipv4_nets = []
    ipv6_nets = []
    
    for item in raw_list:
        s = item.strip()
        # æå– "IP-CIDR, 1.1.1.1/24"
        m = re.match(r'^(?:ip(?:-)?cidr6?|ip6|ip)\s*[:,]?\s*([^,\s]+)', s, re.IGNORECASE)
        if m: s = m.group(1)
            
        try:
            # strict=False å…è®¸ä¸»æœºä½ä¸ä¸º0çš„å†™æ³•
            net = ipaddress.ip_network(s, strict=False)
            if net.version == 4:
                ipv4_nets.append(net)
            else:
                ipv6_nets.append(net)
        except ValueError:
            continue

    # æ ¸å¿ƒåŠŸèƒ½ï¼šåˆå¹¶ç½‘æ®µ (ä¾‹å¦‚ 1.1.1.1/32 + 1.1.1.0/32 -> æ— éœ€åˆå¹¶ï¼Œæˆ–ç›¸é‚»åˆå¹¶)
    try:
        merged_v4 = list(ipaddress.collapse_addresses(ipv4_nets))
        merged_v6 = list(ipaddress.collapse_addresses(ipv6_nets))
        return [str(n) for n in merged_v4 + merged_v6]
    except Exception as e:
        # ä¸‡ä¸€åˆå¹¶å‡ºé”™ï¼Œå›é€€åˆ°åŒ…å«é‡å¤çš„åˆ—è¡¨
        return sorted([str(n) for n in ipv4_nets + ipv6_nets])

# =========================
# é€‚é…å·¥ä½œæµçš„æ–° Main å‡½æ•°
# =========================

def process_file(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # 1. åˆæ­¥è§£æ (é€šç”¨)
        raw_list = parse_content_to_list(content)
        
        # 2. åˆ¤æ–­å¤„ç†æ¨¡å¼ (æ ¹æ®è·¯å¾„åˆ¤æ–­ IP è¿˜æ˜¯ Domain)
        # è§„åˆ™ç»“æ„é€šå¸¸ä¸º: rulesets/block/domain/owner/file.txt
        # æˆ–è€… rulesets/direct/ipcidr/owner/file.txt
        # æˆ‘ä»¬æ£€æµ‹è·¯å¾„ä¸­æ˜¯å¦åŒ…å« 'ipcidr' æˆ– 'ip'ï¼Œå¦åˆ™é»˜è®¤ä¸º domain
        
        path_lower = filepath.lower()
        is_ip_mode = 'ipcidr' in path_lower or '/ip/' in path_lower
        
        if is_ip_mode:
            final_list = process_ip_list(raw_list)
        else:
            final_list = process_domain_list(raw_list)
            
        # 3. å†™å›
        with open(filepath, 'w', encoding='utf-8') as f:
            for line in final_list:
                f.write(line + "\n")
        
        STATS["files_processed"] += 1
        STATS["valid_lines"] += len(final_list)

    except UnicodeDecodeError:
        gh_error(f"Encoding error", file=filepath)
    except Exception as e:
        gh_error(f"Process error: {e}", file=filepath)

def main():
    print("::notice::Starting Smart Rule Processor (Base64/YAML/CIDR-Merge)...")
    
    if not os.path.exists(SOURCE_DIR):
        print(f"::warning::Directory '{SOURCE_DIR}' not found.")
        return

    gh_group_start(f"Processing {SOURCE_DIR}")
    
    # æ‰«ææ–‡ä»¶
    target_files = []
    for root, dirs, files in os.walk(SOURCE_DIR):
        for file in files:
            if file.endswith(('.txt', '.list', '.conf', '.yaml')):
                target_files.append(os.path.join(root, file))
    
    print_step(f"Found {len(target_files)} files.")
    
    # æ‰§è¡Œå¤„ç†
    for fp in target_files:
        process_file(fp)
        
    gh_group_end()

    # è¾“å‡ºæŠ¥å‘Š
    removed_total = STATS["original_lines"] - STATS["valid_lines"]
    print_success("Sanitization & Optimization Complete.")
    print(f"  - Files: {STATS['files_processed']}")
    print(f"  - Base64 Decoded: {STATS['base64_decoded']}")
    print(f"  - Lines Kept: {STATS['valid_lines']}")
    print(f"  - Lines Reduced: {removed_total}")

    if os.getenv('GITHUB_STEP_SUMMARY'):
        with open(os.getenv('GITHUB_STEP_SUMMARY'), 'a', encoding='utf-8') as f:
            f.write("## ğŸ§  Intelligent Processor Report\n")
            f.write(f"- **Files Processed**: `{STATS['files_processed']}`\n")
            f.write(f"- **Base64 Sources Decoded**: `{STATS['base64_decoded']}`\n")
            f.write(f"- **Cleaned Rules**: `{STATS['valid_lines']}`\n")
            f.write(f"- **Reduction**: `{removed_total}` lines removed (duplicates, invalid, or aggregated CIDRs)\n")

if __name__ == '__main__':
    main()
